{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results from:\n",
      " - word2vec_results_500_1000.json\n",
      " - word2vec_results_10_20_50.json\n",
      " - word2vec_results_100_250.json\n",
      "\n",
      "Number of assesed word2vec models: 48\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# result path\n",
    "result_path = 'results/'\n",
    "name_start = 'word2vec_results_'\n",
    "name_end = '.json'\n",
    "\n",
    "\n",
    "if not os.path.exists(result_path) or not os.path.isfile(result_path + 'final_word2vec_results.json'):\n",
    "    try:\n",
    "        os.makedirs(result_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    final_results = dict()\n",
    "\n",
    "    print('Loading results from:')\n",
    "\n",
    "    for file in os.listdir(result_path):\n",
    "        if file.startswith(name_start) and file.endswith(name_end):\n",
    "            print(f' - {file}')\n",
    "            with open(result_path + file) as f:\n",
    "                data = json.load(f)\n",
    "                for key, value in data.items():\n",
    "                    final_results[key] = value\n",
    "\n",
    "    print('')\n",
    "    print(f\"Number of assesed word2vec models: {len(final_results)}\")\n",
    "\n",
    "\n",
    "    with open(result_path + 'final_word2vec_results.json', 'w') as fp:\n",
    "        json.dump(final_results, fp)\n",
    "else:\n",
    "    print('Final results already exist, skipping...')\n",
    "    with open(result_path + 'final_word2vec_results.json') as f:\n",
    "        final_results = json.load(f)\n",
    "    print('Imported final results from: ' + result_path + 'final_word2vec_results.json')\n",
    "    print(f\"Number of assesed word2vec models: {len(final_results)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy as cp\n",
    "import numpy as np\n",
    "word2vec_results = cp.deepcopy(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each word2vec model calculate mean auc\n",
    "word2vec_mean_auc = {}\n",
    "\n",
    "for model in word2vec_results:\n",
    "    # get results for each model\n",
    "    results = word2vec_results[model]\n",
    "    # get auc for each model\n",
    "    aucs = [results[model]['auc'] for model in results]\n",
    "    # calculate mean auc\n",
    "    mean_auc = np.mean(aucs)\n",
    "    # add to dict\n",
    "    word2vec_mean_auc[model] = mean_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort dict by mean auc in descending order\n",
    "word2vec_mean_auc = dict(sorted(word2vec_mean_auc.items(), key=lambda item: item[1], reverse=True))\n",
    "word2vec_mean_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each word2vec model calculate mean auc\n",
    "word2vec_mean_f1 = {}\n",
    "\n",
    "for model in word2vec_results:\n",
    "    # get results for each model\n",
    "    results = word2vec_results[model]\n",
    "    # get auc for each model\n",
    "    f1s = [results[model]['f1'] for model in results]\n",
    "    # calculate mean auc\n",
    "    mean_f1 = np.mean(f1s)\n",
    "    # add to dict\n",
    "    word2vec_mean_f1[model] = mean_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort dict by mean f1 in descending order\n",
    "word2vec_mean_f1 = dict(sorted(word2vec_mean_f1.items(), key=lambda item: item[1], reverse=True))\n",
    "word2vec_mean_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word2vec_vs10_win3_sg1',\n",
       " 'word2vec_vs10_win5_sg0',\n",
       " 'word2vec_vs10_win5_sg1',\n",
       " 'word2vec_vs10_win6_sg0',\n",
       " 'word2vec_vs10_win8_sg0',\n",
       " 'word2vec_vs20_win4_sg1',\n",
       " 'word2vec_vs20_win5_sg1',\n",
       " 'word2vec_vs20_win6_sg0',\n",
       " 'word2vec_vs20_win8_sg0'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get first 10 models by auc and their mean auc\n",
    "top_10_auc = dict(list(word2vec_mean_auc.items())[:10])\n",
    "\n",
    "# get first 10 models by f1 and their mean f1\n",
    "top_10_f1 = dict(list(word2vec_mean_f1.items())[:10])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get intersection of top 10 models by auc and f1\n",
    "top_10_auc_f1 = set(top_10_auc.keys()).intersection(set(top_10_f1.keys()))\n",
    "top_10_auc_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word2vec_vs20_win4_sg1': 0.947655956438699,\n",
       " 'word2vec_vs10_win5_sg0': 0.9470629695591175,\n",
       " 'word2vec_vs10_win3_sg1': 0.9464177322420774,\n",
       " 'word2vec_vs20_win8_sg0': 0.9447619492303622,\n",
       " 'word2vec_vs10_win6_sg0': 0.9445193737951827,\n",
       " 'word2vec_vs10_win8_sg0': 0.9440878074160046,\n",
       " 'word2vec_vs20_win5_sg1': 0.943381648312311,\n",
       " 'word2vec_vs10_win5_sg1': 0.9428973868111002,\n",
       " 'word2vec_vs10_win7_sg0': 0.9425769915754507,\n",
       " 'word2vec_vs20_win6_sg0': 0.9425716553605614}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word2vec_vs20_win4_sg1': 0.9495703290060525,\n",
       " 'word2vec_vs10_win5_sg0': 0.948653103782284,\n",
       " 'word2vec_vs10_win3_sg1': 0.9485795996240907,\n",
       " 'word2vec_vs20_win8_sg0': 0.9471045947401752,\n",
       " 'word2vec_vs10_win6_sg0': 0.9462215600767129,\n",
       " 'word2vec_vs20_win5_sg1': 0.9458119417631338,\n",
       " 'word2vec_vs10_win8_sg0': 0.9454863388177343,\n",
       " 'word2vec_vs10_win5_sg1': 0.9453271430102415,\n",
       " 'word2vec_vs20_win6_sg0': 0.9447833869556108,\n",
       " 'word2vec_vs10_win6_sg1': 0.944464850329604}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WINNER - `word2vec_vs20_win4_sg1`\n",
    "Since it was on average the best performing model for both auc and f1 on different predictive models, we will use this model to generate the word embeddings for the rest of the project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
