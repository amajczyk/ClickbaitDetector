{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "%run '../functions.py'\n",
    "%run '../classes.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'grid_search_results_new.json'\n",
    "grids_path = '../grid_search_grids.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_settings = return_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_settings['model_path'] = '../all_datasets/' + model_w2v_settings['model_path']\n",
    "model_w2v = Word2VecModel(model_w2v_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read preprocessed data from pickle file\n",
    "df = pd.read_pickle('data/preprocessed_titles_labels.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample\n",
       "train    17791\n",
       "test       997\n",
       "val2       506\n",
       "val1       480\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sample'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test stratified by y\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "df['title_vector'] = [get_word_vectors(model_w2v, title, aggregation='mean') for title in df['title']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([72,\n",
       "  2,\n",
       "  83,\n",
       "  71,\n",
       "  7,\n",
       "  26,\n",
       "  77,\n",
       "  21,\n",
       "  94,\n",
       "  74,\n",
       "  29,\n",
       "  46,\n",
       "  39,\n",
       "  3,\n",
       "  89,\n",
       "  70,\n",
       "  22,\n",
       "  90,\n",
       "  19,\n",
       "  58,\n",
       "  99,\n",
       "  9,\n",
       "  32,\n",
       "  86,\n",
       "  13,\n",
       "  11,\n",
       "  57,\n",
       "  41,\n",
       "  81,\n",
       "  35,\n",
       "  85,\n",
       "  62,\n",
       "  30,\n",
       "  47,\n",
       "  36,\n",
       "  31,\n",
       "  1,\n",
       "  59,\n",
       "  40,\n",
       "  64,\n",
       "  65,\n",
       "  91,\n",
       "  76,\n",
       "  4,\n",
       "  44,\n",
       "  45,\n",
       "  33,\n",
       "  51,\n",
       "  67,\n",
       "  5,\n",
       "  6,\n",
       "  96,\n",
       "  88,\n",
       "  98,\n",
       "  54,\n",
       "  38,\n",
       "  84,\n",
       "  92,\n",
       "  66,\n",
       "  28,\n",
       "  48,\n",
       "  52,\n",
       "  34,\n",
       "  63,\n",
       "  23,\n",
       "  17,\n",
       "  68,\n",
       "  49,\n",
       "  14,\n",
       "  10],\n",
       " 70)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables_to_drop = get_dimensions_to_drop()\n",
    "variables_to_drop, len(variables_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_vector'] = [drop_dimensions_from_vector(vector, variables_to_drop) for vector in df['title_vector']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.vstack(df[df['sample']=='train']['title_vector'])\n",
    "X_test = np.vstack(df[df['sample']=='val2']['title_vector'])\n",
    "\n",
    "y_train = df[df['sample']=='train']['is_clickbait']\n",
    "y_test = df[df['sample']=='val2']['is_clickbait']\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(grids_path, 'r') as f:\n",
    "    model_settings = json.load(f)\n",
    "\n",
    "model_settings\n",
    "grids = model_settings['grid_search_grids']\n",
    "grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decision_tree', 'catboost', 'lightgbm', 'xgboost', 'random_forest']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_to_CV = list()\n",
    "\n",
    "for model_name, grid in grids.items():\n",
    "    # check if model has already been trained - if 'best_params' exists in grid\n",
    "    if 'best_params' in grid.keys():\n",
    "        print(f'Model {model_name} already trained')\n",
    "        continue\n",
    "    models_to_CV.append((model_name))\n",
    "models_to_CV\n",
    "\n",
    "# remove _grid from model names\n",
    "models_to_CV = [model_name.replace('_grid', '') for model_name in models_to_CV]\n",
    "models_to_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_model_with_param(model_name, param = dict()):\n",
    "    if model_name == 'decision_tree':\n",
    "        model = DecisionTreeClassifier(**param)\n",
    "    elif model_name == 'random_forest':\n",
    "        model = RandomForestClassifier(**param)\n",
    "    elif model_name == 'xgboost':\n",
    "        model = XGBClassifier(**param)\n",
    "    elif model_name == 'lightgbm':\n",
    "        model = LGBMClassifier(**param, verbose=-1)\n",
    "    elif model_name == 'catboost':\n",
    "        model = CatBoostClassifier(**param,verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_CV = [\n",
    "\n",
    "    'lightgbm', \n",
    "    'xgboost',\n",
    "    'decision_tree', \n",
    "    'random_forest',  \n",
    "    'catboost'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# read existing results\n",
    "try:\n",
    "    with open(save_path, 'r') as f:\n",
    "        grid_search_results = json.load(f)\n",
    "    print('Loaded existing results')\n",
    "except:\n",
    "    grid_search_results = dict()\n",
    "    print('No existing results found - creating new dict')\n",
    "for model_name in models_to_CV:\n",
    "    grid_search_results[model_name] = dict()\n",
    "\n",
    "for model_name in tqdm(models_to_CV, desc = 'Models'):\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # get grid\n",
    "    grid = grids[model_name+'_grid']\n",
    "\n",
    "    # generate all combinations of parameters\n",
    "    import itertools\n",
    "\n",
    "    keys, values = zip(*grid.items())\n",
    "    combinations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    # do grid search\n",
    "    \n",
    "    for params in combinations_dicts:\n",
    "        grid_search_results[model_name][str(params)] = dict()\n",
    "\n",
    "    for params in tqdm(combinations_dicts, desc = 'Grid combinations search for model {}'.format(model_name)):\n",
    "       \n",
    "        model = return_model_with_param(model_name, params)\n",
    "        scores_auc_cv_val = list()\n",
    "        scores_auc_val2 = list()\n",
    "        scores_auc_train = list()\n",
    "\n",
    "        scores_f1_cv_val = list()\n",
    "        scores_f1_val2 = list()\n",
    "        scores_f1_train = list()\n",
    "\n",
    "        for train_index, val_index in kf.split(X_train_scaled, y_train):\n",
    "            X_train_kf = X_train_scaled[train_index]\n",
    "            X_val_kf = X_train_scaled[val_index]\n",
    "            y_train_kf = y_train.iloc[train_index]\n",
    "            y_val_kf = y_train.iloc[val_index]\n",
    "\n",
    "            model.fit(X_train_kf, y_train_kf)\n",
    "\n",
    "            # print(model)\n",
    "\n",
    "        \n",
    "            scores_auc_cv_val.append(roc_auc_score(y_val_kf, model.predict_proba(X_val_kf)[:,1]))\n",
    "            scores_auc_val2.append(roc_auc_score(y_test, model.predict_proba(X_test_scaled)[:,1]))\n",
    "            scores_auc_train.append(roc_auc_score(y_train_kf, model.predict_proba(X_train_kf)[:,1]))\n",
    "\n",
    "            y_pred_cv_val = model.predict_proba(X_val_kf)[:,1]\n",
    "            y_pred_val2 = model.predict_proba(X_test_scaled)[:,1]\n",
    "            y_pred_train = model.predict_proba(X_train_kf)[:,1]\n",
    "\n",
    "            # print(y_pred_cv_val[y_pred_cv_val>0.5])\n",
    "\n",
    "            y_pred_cv_val = np.where(y_pred_cv_val > 0.5, 1, 0)\n",
    "            y_pred_val2 = np.where(y_pred_val2 > 0.5, 1, 0)\n",
    "            y_pred_train = np.where(y_pred_train > 0.5, 1, 0)\n",
    "\n",
    "\n",
    "            \n",
    "            scores_f1_cv_val.append(f1_score(y_val_kf, model.predict(X_val_kf)))\n",
    "            scores_f1_val2.append(f1_score(y_test, model.predict(X_test_scaled)))\n",
    "            scores_f1_train.append(f1_score(y_train_kf, model.predict(X_train_kf)))\n",
    "            break\n",
    "\n",
    "        # print(scores_f1_train, scores_f1_cv_val, scores_f1_val2)\n",
    "        # print(scores_auc_train, scores_auc_cv_val, scores_auc_val2)\n",
    "\n",
    "        grid_search_results[model_name][str(params)]['mean_train_f1'] = np.mean(scores_f1_train)\n",
    "        grid_search_results[model_name][str(params)]['mean_cv_val_f1'] = np.mean(scores_f1_cv_val)\n",
    "        grid_search_results[model_name][str(params)]['mean_val2_f1'] = np.mean(scores_f1_val2)\n",
    "\n",
    "        grid_search_results[model_name][str(params)]['mean_train_auc'] = np.mean(scores_auc_train)\n",
    "        grid_search_results[model_name][str(params)]['mean_cv_val_auc'] = np.mean(scores_auc_cv_val)\n",
    "        grid_search_results[model_name][str(params)]['mean_val2_auc'] = np.mean(scores_auc_val2)\n",
    "\n",
    "        grid_search_results[model_name][str(params)]['f1_diff_cv_val'] = np.mean(scores_f1_train) - np.mean(scores_f1_cv_val)\n",
    "        grid_search_results[model_name][str(params)]['auc_diff_cv_val'] = np.mean(scores_auc_train) - np.mean(scores_auc_cv_val)\n",
    "\n",
    "        grid_search_results[model_name][str(params)]['f1_diff_val2'] = np.mean(scores_f1_train) - np.mean(scores_f1_val2)\n",
    "        grid_search_results[model_name][str(params)]['auc_diff_val2'] = np.mean(scores_auc_train) - np.mean(scores_auc_val2)\n",
    "\n",
    "        grid_search_results[model_name][str(params)]['params'] = params\n",
    "\n",
    "        # save results to json file\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(grid_search_results, f)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    print(grid_search_results[model_name])\n",
    "    \n",
    "# grid_search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "with open(save_path, 'w') as f:\n",
    "    json.dump(grid_search_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results\n",
    "with open(save_path, 'r') as f:\n",
    "    grid_search_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res= pd.DataFrame()\n",
    "\n",
    "for model in grid_search_results:\n",
    "    temp = grid_search_results[model]\n",
    "    temp = pd.DataFrame.from_dict(temp)\n",
    "    temp = temp.transpose()\n",
    "    temp['model'] = model\n",
    "    df_res = pd.concat([df_res, temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = df_res.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_auc = df_res[(df_res['auc_diff_val2']<=0.05)].sort_values(by='mean_val2_auc', ascending=False).head(500)\n",
    "top_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topf1 = df_res[(df_res['f1_diff_val2']<=0.05)].sort_values(by='mean_val2_f1', ascending=False).head(500)\n",
    "topf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_both = pd.merge(top_auc, topf1, on=['index'], how='inner')\n",
    "\n",
    "# keep _x columns\n",
    "top_both = top_both[[col for col in top_both.columns if '_x' in col]]\n",
    "top_both.columns = [col.replace('_x', '') for col in top_both.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_both['sum_diff_val2'] = top_both['f1_diff_val2'] + top_both['auc_diff_val2']\n",
    "top_both['sum_auc_f1_val2'] = top_both['mean_val2_f1'] + top_both['mean_val2_auc']\n",
    "\n",
    "top_both = top_both.sort_values(by='sum_auc_f1_val2', ascending=False).head(1000).reset_index(drop=True)\n",
    "top_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner model: |catboost| with params: {'iterations': 100, 'learning_rate': 0.2, 'depth': 5, 'l2_leaf_reg': 7}\n"
     ]
    }
   ],
   "source": [
    "params, model_name = top_both['params'][0], top_both['model'][0]\n",
    "print('Winner model: |{}|'.format(model_name), 'with params: {}'.format(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x288a0e2d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model with best params\n",
    "model = return_model_with_param(model_name, params)\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import pickle\n",
    "with open('predictive_models/{}.pkl'.format(model_name), 'wb') as f:\n",
    "    pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save scaler\n",
    "with open('predictive_models/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8450148075024678\n",
      "AUC score: 0.9233808983428704\n"
     ]
    }
   ],
   "source": [
    "# calculate metrics on test set\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "X_test = np.vstack(df[df['sample']=='test']['title_vector'])\n",
    "y_test = df[df['sample']=='test']['is_clickbait']\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "print('F1 score: {}'.format(f1_score(y_test, y_pred)))\n",
    "print('AUC score: {}'.format(roc_auc_score(y_test, y_pred_proba)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best threshold for f1 score\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "# find threshold for best f1 score\n",
    "thresholds = np.linspace(0, 1, 10000)\n",
    "f1s = dict()\n",
    "for threshold in tqdm(thresholds):\n",
    "    y_pred = (model.predict_proba(X_test_scaled)[:, 1] >= threshold).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    f1s[threshold] = f1\n",
    "\n",
    "# get threshold for best f1 score\n",
    "threshold = max(f1s, key=f1s.get)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC test: 0.9233808983428704\n",
      "F1 test: 0.860093896713615\n"
     ]
    }
   ],
   "source": [
    "# find auc and f1 for best threshold\n",
    "y_pred = (model.predict_proba(X_test_scaled)[:, 1] >= threshold).astype(int)\n",
    "y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'ROC AUC test: {roc_auc}')\n",
    "print(f'F1 test: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC train: 0.9563398826157218\n",
      "F1 train: 0.8833788376887501\n"
     ]
    }
   ],
   "source": [
    "# find auc and f1 for best threshold on train\n",
    "y_pred = (model.predict_proba(X_train_scaled)[:, 1] >= threshold).astype(int)\n",
    "y_proba = model.predict_proba(X_train_scaled)[:, 1]\n",
    "\n",
    "roc_auc = roc_auc_score(y_train, y_proba)\n",
    "f1 = f1_score(y_train, y_pred)\n",
    "\n",
    "print(f'ROC AUC train: {roc_auc}')\n",
    "print(f'F1 train: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save threshold to txt file\n",
    "with open('predictive_models/threshold.txt', 'w') as f:\n",
    "    f.write(str(threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dim_0', 'dim_8', 'dim_12', 'dim_15', 'dim_16', 'dim_18', 'dim_20',\n",
       "       'dim_24', 'dim_25', 'dim_27', 'dim_37', 'dim_42', 'dim_43',\n",
       "       'dim_50', 'dim_53', 'dim_55', 'dim_56', 'dim_60', 'dim_61',\n",
       "       'dim_69', 'dim_73', 'dim_75', 'dim_78', 'dim_79', 'dim_80',\n",
       "       'dim_82', 'dim_87', 'dim_93', 'dim_95', 'dim_97'], dtype='<U6')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_undropped_dimensions(dropped_dimensions, n_dim):\n",
    "    all_dimensions = list(range(n_dim))\n",
    "    undropped_dimensions = [dim for dim in all_dimensions if dim not in dropped_dimensions]\n",
    "    return undropped_dimensions\n",
    "\n",
    "undropped_dimensions = get_undropped_dimensions(variables_to_drop, 100)\n",
    "undropped_dimensions = [str('dim_'+str(dim)) for dim in undropped_dimensions]\n",
    "undropped_dimensions = np.array(undropped_dimensions)\n",
    "undropped_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=undropped_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparation of a new explainer is initiated\n",
      "\n",
      "  -> data              : 17791 rows 30 cols\n",
      "  -> target variable   : Parameter 'y' was a pandas.Series. Converted to a numpy.ndarray.\n",
      "  -> target variable   : 17791 values\n",
      "  -> model_class       : catboost.core.CatBoostClassifier (default)\n",
      "  -> label             : Champion Model\n",
      "  -> predict function  : <function yhat_proba_default at 0x2cde896c0> will be used (default)\n",
      "  -> predict function  : Accepts pandas.DataFrame and numpy.ndarray.\n",
      "  -> predicted values  : min = 0.000434, mean = 0.475, max = 1.0\n",
      "  -> model type        : classification will be used (default)\n",
      "  -> residual function : difference between y and yhat (default)\n",
      "  -> residuals         : min = -0.995, mean = -0.000225, max = 0.998\n",
      "  -> model_info        : package catboost\n",
      "\n",
      "A new explainer has been created!\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "displaylogo": false,
        "modeBarButtonsToRemove": [
         "sendDataToCloud",
         "lasso2d",
         "autoScale2d",
         "select2d",
         "zoom2d",
         "pan2d",
         "zoomIn2d",
         "zoomOut2d",
         "resetScale2d",
         "toggleSpikelines",
         "hoverCompareCartesian",
         "hoverClosestCartesian"
        ],
        "plotlyServerURL": "https://plot.ly",
        "staticPlot": false,
        "toImageButtonOptions": {
         "height": null,
         "width": null
        }
       },
       "data": [
        {
         "base": 0.04505581635907761,
         "hoverinfo": "text",
         "hoverlabel": {
          "bgcolor": "rgba(0,0,0,0.8)"
         },
         "hovertext": [
          "Model: Champion Model loss after<br>variable: dim_24 is permuted: 0.071<br>Drop-out loss change: +0.026",
          "Model: Champion Model loss after<br>variable: dim_8 is permuted: 0.063<br>Drop-out loss change: +0.018",
          "Model: Champion Model loss after<br>variable: dim_80 is permuted: 0.06<br>Drop-out loss change: +0.015",
          "Model: Champion Model loss after<br>variable: dim_60 is permuted: 0.053<br>Drop-out loss change: +0.008",
          "Model: Champion Model loss after<br>variable: dim_37 is permuted: 0.053<br>Drop-out loss change: +0.008",
          "Model: Champion Model loss after<br>variable: dim_69 is permuted: 0.053<br>Drop-out loss change: +0.008",
          "Model: Champion Model loss after<br>variable: dim_87 is permuted: 0.052<br>Drop-out loss change: +0.007",
          "Model: Champion Model loss after<br>variable: dim_55 is permuted: 0.051<br>Drop-out loss change: +0.006",
          "Model: Champion Model loss after<br>variable: dim_61 is permuted: 0.051<br>Drop-out loss change: +0.006",
          "Model: Champion Model loss after<br>variable: dim_42 is permuted: 0.051<br>Drop-out loss change: +0.006",
          "Model: Champion Model loss after<br>variable: dim_97 is permuted: 0.05<br>Drop-out loss change: +0.005",
          "Model: Champion Model loss after<br>variable: dim_95 is permuted: 0.05<br>Drop-out loss change: +0.005",
          "Model: Champion Model loss after<br>variable: dim_43 is permuted: 0.05<br>Drop-out loss change: +0.005",
          "Model: Champion Model loss after<br>variable: dim_82 is permuted: 0.05<br>Drop-out loss change: +0.005",
          "Model: Champion Model loss after<br>variable: dim_25 is permuted: 0.05<br>Drop-out loss change: +0.005",
          "Model: Champion Model loss after<br>variable: dim_0 is permuted: 0.05<br>Drop-out loss change: +0.005",
          "Model: Champion Model loss after<br>variable: dim_18 is permuted: 0.05<br>Drop-out loss change: +0.004",
          "Model: Champion Model loss after<br>variable: dim_75 is permuted: 0.049<br>Drop-out loss change: +0.004",
          "Model: Champion Model loss after<br>variable: dim_15 is permuted: 0.049<br>Drop-out loss change: +0.004",
          "Model: Champion Model loss after<br>variable: dim_93 is permuted: 0.049<br>Drop-out loss change: +0.004",
          "Model: Champion Model loss after<br>variable: dim_56 is permuted: 0.049<br>Drop-out loss change: +0.004",
          "Model: Champion Model loss after<br>variable: dim_20 is permuted: 0.049<br>Drop-out loss change: +0.004",
          "Model: Champion Model loss after<br>variable: dim_16 is permuted: 0.048<br>Drop-out loss change: +0.003",
          "Model: Champion Model loss after<br>variable: dim_50 is permuted: 0.048<br>Drop-out loss change: +0.003",
          "Model: Champion Model loss after<br>variable: dim_27 is permuted: 0.048<br>Drop-out loss change: +0.003",
          "Model: Champion Model loss after<br>variable: dim_79 is permuted: 0.048<br>Drop-out loss change: +0.003",
          "Model: Champion Model loss after<br>variable: dim_12 is permuted: 0.048<br>Drop-out loss change: +0.003",
          "Model: Champion Model loss after<br>variable: dim_53 is permuted: 0.047<br>Drop-out loss change: +0.002",
          "Model: Champion Model loss after<br>variable: dim_78 is permuted: 0.047<br>Drop-out loss change: +0.002",
          "Model: Champion Model loss after<br>variable: dim_73 is permuted: 0.047<br>Drop-out loss change: +0.001"
         ],
         "marker": {
          "color": "#46bac2"
         },
         "orientation": "h",
         "showlegend": false,
         "text": [
          "+0.026",
          "+0.018",
          "+0.015",
          "+0.008",
          "+0.008",
          "+0.008",
          "+0.007",
          "+0.006",
          "+0.006",
          "+0.006",
          "+0.005",
          "+0.005",
          "+0.005",
          "+0.005",
          "+0.005",
          "+0.005",
          "+0.004",
          "+0.004",
          "+0.004",
          "+0.004",
          "+0.004",
          "+0.004",
          "+0.003",
          "+0.003",
          "+0.003",
          "+0.003",
          "+0.003",
          "+0.002",
          "+0.002",
          "+0.001"
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          0.026181784102607283,
          0.01769133615261917,
          0.014975226380689391,
          0.008389100792324758,
          0.00800729690913507,
          0.00792702057462135,
          0.006713163398976332,
          0.006304185564485379,
          0.0062316004240733705,
          0.006125198983880191,
          0.005103260062656635,
          0.004898399994256911,
          0.004830387386534275,
          0.004791774135655728,
          0.004742477398814948,
          0.0046686201256393375,
          0.004457882719709483,
          0.004009452878223921,
          0.0038132080910528235,
          0.003749060612260359,
          0.0036410824977230957,
          0.0035746246147065827,
          0.0032325959792450926,
          0.003021026070237111,
          0.0029675324434810524,
          0.00284947527363294,
          0.0027195871573635483,
          0.0023056146864918747,
          0.0022656710884424447,
          0.001483848641533038
         ],
         "xaxis": "x",
         "y": [
          "dim_24",
          "dim_8",
          "dim_80",
          "dim_60",
          "dim_37",
          "dim_69",
          "dim_87",
          "dim_55",
          "dim_61",
          "dim_42",
          "dim_97",
          "dim_95",
          "dim_43",
          "dim_82",
          "dim_25",
          "dim_0",
          "dim_18",
          "dim_75",
          "dim_15",
          "dim_93",
          "dim_56",
          "dim_20",
          "dim_16",
          "dim_50",
          "dim_27",
          "dim_79",
          "dim_12",
          "dim_53",
          "dim_78",
          "dim_73"
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Champion Model",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "drop-out loss",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0,
          "yanchor": "top",
          "yref": "paper",
          "yshift": -30
         }
        ],
        "font": {
         "color": "#371ea3"
        },
        "height": 783,
        "margin": {
         "b": 71,
         "r": 30,
         "t": 78
        },
        "shapes": [
         {
          "line": {
           "color": "#371ea3",
           "dash": "dot",
           "width": 1.5
          },
          "type": "line",
          "x0": 0.04505581635907761,
          "x1": 0.04505581635907761,
          "xref": "x",
          "y0": -1,
          "y1": 30,
          "yref": "y"
         }
        ],
        "template": {
         "data": {
          "scatter": [
           {
            "type": "scatter"
           }
          ]
         }
        },
        "title": {
         "text": "Variable Importance",
         "x": 0.15
        },
        "xaxis": {
         "anchor": "y",
         "automargin": true,
         "domain": [
          0,
          1
         ],
         "fixedrange": true,
         "gridwidth": 2,
         "range": [
          0.04112854874368652,
          0.075164868077076
         ],
         "tickcolor": "white",
         "ticklen": 3,
         "ticks": "outside",
         "type": "linear",
         "zeroline": false
        },
        "yaxis": {
         "anchor": "x",
         "automargin": true,
         "autorange": "reversed",
         "domain": [
          0,
          1
         ],
         "fixedrange": true,
         "gridwidth": 2,
         "tickcolor": "white",
         "ticklen": 10,
         "ticks": "outside",
         "type": "category"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dalex\n",
    "# Create an explainer object\n",
    "exp = dalex.Explainer(model, X_train_scaled_df, y_train, label='Champion Model')\n",
    "\n",
    "# Calculate feature importance\n",
    "feature_importance = exp.model_parts()\n",
    "feature_importance.plot(max_vars=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
