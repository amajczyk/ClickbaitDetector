{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adammajczyk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adammajczyk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adammajczyk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/adammajczyk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adammajczyk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/adammajczyk/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/adammajczyk/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "%run '../functions.py'\n",
    "%run '../classes.py'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>is_clickbait</th>\n",
       "      <th>text</th>\n",
       "      <th>dataset</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7 Essays To Read: Sex Scenes That Are Actually...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clickbait-dataset</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Horrible Truth About Bagels</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clickbait-dataset</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Entire Dem/Lib Party is Infested with Rats! So...</td>\n",
       "      <td>1</td>\n",
       "      <td>— Susan? (@GaetaSusan) October 27, 2016 WikiLe...</td>\n",
       "      <td>fake-news</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beginning a New Life in America, With Mentors ...</td>\n",
       "      <td>0</td>\n",
       "      <td>OAKLAND, Calif.  —   Pascal Serugendo was only...</td>\n",
       "      <td>fake-news</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Imam pleads guilty in New York subway bomb plot</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clickbait-dataset</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  is_clickbait  \\\n",
       "0  7 Essays To Read: Sex Scenes That Are Actually...             1   \n",
       "1                    The Horrible Truth About Bagels             1   \n",
       "2  Entire Dem/Lib Party is Infested with Rats! So...             1   \n",
       "3  Beginning a New Life in America, With Mentors ...             0   \n",
       "4    Imam pleads guilty in New York subway bomb plot             0   \n",
       "\n",
       "                                                text            dataset sample  \n",
       "0                                                NaN  clickbait-dataset  train  \n",
       "1                                                NaN  clickbait-dataset  train  \n",
       "2  — Susan? (@GaetaSusan) October 27, 2016 WikiLe...          fake-news  train  \n",
       "3  OAKLAND, Calif.  —   Pascal Serugendo was only...          fake-news  train  \n",
       "4                                                NaN  clickbait-dataset  train  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if data not saved as csv, run this\n",
    "import os\n",
    "if not os.path.exists('data/merged_titles_labels.csv'):\n",
    "    df1 = pd.read_csv('../eda/small1/labeled.csv')\n",
    "    df2 = pd.read_csv('../eda/small2/labeled.csv')\n",
    "    df3 = pd.read_csv('../eda/small3/labeled.csv')\n",
    "    df = pd.concat([df1, df2, df3], ignore_index=True).reset_index(drop=True)\n",
    "    df.to_csv('data/merged_titles_labels.csv', index=False)\n",
    "    df.head()\n",
    "else:\n",
    "    df = pd.read_csv('data/merged_titles_labels.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        7 Essays To Read: Sex Scenes That Are Actually...\n",
      "1                          The Horrible Truth About Bagels\n",
      "2        Entire Dem/Lib Party is Infested with Rats! So...\n",
      "3        Beginning a New Life in America, With Mentors ...\n",
      "4          Imam pleads guilty in New York subway bomb plot\n",
      "                               ...                        \n",
      "51798    As Tamil Rebels Lose Ground in Sri Lanka, a Cr...\n",
      "51799    A Lot Of People Are Furious At This Magazine F...\n",
      "51800    11 Reasons Bindi Irwin Is An Incredible Role M...\n",
      "51801    Donald Trump Backslides on Campaign Promise To...\n",
      "51802                           Canadians may elect Senate\n",
      "Name: title, Length: 51803, dtype: object\n",
      "Removing numbers and replacing with words...\n",
      "0        seven Essays To Read: Sex Scenes That Are Actu...\n",
      "1                          The Horrible Truth About Bagels\n",
      "2        Entire Dem/Lib Party is Infested with Rats! So...\n",
      "3        Beginning a New Life in America, With Mentors ...\n",
      "4          Imam pleads guilty in New York subway bomb plot\n",
      "                               ...                        \n",
      "51798    As Tamil Rebels Lose Ground in Sri Lanka, a Cr...\n",
      "51799    A Lot Of People Are Furious At This Magazine F...\n",
      "51800    eleven Reasons Bindi Irwin Is An Incredible Ro...\n",
      "51801    Donald Trump Backslides on Campaign Promise To...\n",
      "51802                           Canadians may elect Senate\n",
      "Name: title, Length: 51803, dtype: object\n",
      "Removing possesive s...\n",
      "0        seven Essays To Read: Sex Scenes That Are Actu...\n",
      "1                          The Horrible Truth About Bagels\n",
      "2        Entire Dem/Lib Party is Infested with Rats! So...\n",
      "3        Beginning a New Life in America, With Mentors ...\n",
      "4          Imam pleads guilty in New York subway bomb plot\n",
      "                               ...                        \n",
      "51798    As Tamil Rebels Lose Ground in Sri Lanka, a Cr...\n",
      "51799    A Lot Of People Are Furious At This Magazine F...\n",
      "51800    eleven Reasons Bindi Irwin Is An Incredible Ro...\n",
      "51801    Donald Trump Backslides on Campaign Promise To...\n",
      "51802                           Canadians may elect Senate\n",
      "Name: title, Length: 51803, dtype: object\n",
      "Expanding short versions...\n",
      "0        seven Essays To Read: Sex Scenes That Are Actu...\n",
      "1                          The Horrible Truth About Bagels\n",
      "2        Entire Dem/Lib Party is Infested with Rats! So...\n",
      "3        Beginning a New Life in America, With Mentors ...\n",
      "4          Imam pleads guilty in New York subway bomb plot\n",
      "                               ...                        \n",
      "51798    As Tamil Rebels Lose Ground in Sri Lanka, a Cr...\n",
      "51799    A Lot Of People Are Furious At This Magazine F...\n",
      "51800    eleven Reasons Bindi Irwin Is An Incredible Ro...\n",
      "51801    Donald Trump Backslides on Campaign Promise To...\n",
      "51802                           Canadians may elect Senate\n",
      "Name: title, Length: 51803, dtype: object\n",
      "Removing punctuation...\n",
      "0        seven Essays To Read Sex Scenes That Are Actua...\n",
      "1                          The Horrible Truth About Bagels\n",
      "2        Entire DemLib Party is Infested with Rats So m...\n",
      "3        Beginning a New Life in America With Mentors b...\n",
      "4          Imam pleads guilty in New York subway bomb plot\n",
      "                               ...                        \n",
      "51798    As Tamil Rebels Lose Ground in Sri Lanka a Cri...\n",
      "51799    A Lot Of People Are Furious At This Magazine F...\n",
      "51800    eleven Reasons Bindi Irwin Is An Incredible Ro...\n",
      "51801    Donald Trump Backslides on Campaign Promise To...\n",
      "51802                           Canadians may elect Senate\n",
      "Name: title, Length: 51803, dtype: object\n",
      "Replacing US with USA...\n",
      "0        seven Essays To Read Sex Scenes That Are Actua...\n",
      "1                          The Horrible Truth About Bagels\n",
      "2        Entire DemLib Party is Infested with Rats So m...\n",
      "3        Beginning a New Life in America With Mentors b...\n",
      "4          Imam pleads guilty in New York subway bomb plot\n",
      "                               ...                        \n",
      "51798    As Tamil Rebels Lose Ground in Sri Lanka a Cri...\n",
      "51799    A Lot Of People Are Furious At This Magazine F...\n",
      "51800    eleven Reasons Bindi Irwin Is An Incredible Ro...\n",
      "51801    Donald Trump Backslides on Campaign Promise To...\n",
      "51802                           Canadians may elect Senate\n",
      "Name: title, Length: 51803, dtype: object\n",
      "Tokenizing...\n",
      "0        [seven, essays, read, sex, scenes, actually, s...\n",
      "1                                [horrible, truth, bagels]\n",
      "2        [entire, demlib, party, infested, rats, much, ...\n",
      "3        [beginning, new, life, america, mentors, side,...\n",
      "4        [imam, pleads, guilty, new, york, subway, bomb...\n",
      "                               ...                        \n",
      "51798    [tamil, rebels, lose, ground, sri, lanka, cris...\n",
      "51799    [lot, people, furious, magazine, shaming, wome...\n",
      "51800    [eleven, reasons, bindi, irwin, incredible, ro...\n",
      "51801    [donald, trump, backslides, campaign, promise,...\n",
      "51802                      [canadians, may, elect, senate]\n",
      "Name: title, Length: 51803, dtype: object\n",
      "Lemmalizing words...\n",
      "0        [seven, essay, read, sex, scene, actually, sex...\n",
      "1                                 [horrible, truth, bagel]\n",
      "2        [entire, demlib, party, infested, rat, much, c...\n",
      "3        [beginning, new, life, america, mentor, side, ...\n",
      "4        [imam, pleads, guilty, new, york, subway, bomb...\n",
      "                               ...                        \n",
      "51798    [tamil, rebel, lose, ground, sri, lanka, crisi...\n",
      "51799    [lot, people, furious, magazine, shaming, woma...\n",
      "51800    [eleven, reason, bindi, irwin, incredible, rol...\n",
      "51801    [donald, trump, backslides, campaign, promise,...\n",
      "51802                       [canadian, may, elect, senate]\n",
      "Name: title, Length: 51803, dtype: object\n",
      "Removing non ascii characters...\n",
      "0        [seven, essay, read, sex, scene, actually, sex...\n",
      "1                                 [horrible, truth, bagel]\n",
      "2        [entire, demlib, party, infested, rat, much, c...\n",
      "3        [beginning, new, life, america, mentor, side, ...\n",
      "4        [imam, pleads, guilty, new, york, subway, bomb...\n",
      "                               ...                        \n",
      "51798    [tamil, rebel, lose, ground, sri, lanka, crisi...\n",
      "51799    [lot, people, furious, magazine, shaming, woma...\n",
      "51800    [eleven, reason, bindi, irwin, incredible, rol...\n",
      "51801    [donald, trump, backslides, campaign, promise,...\n",
      "51802                       [canadian, may, elect, senate]\n",
      "Name: title, Length: 51803, dtype: object\n",
      "Removing empty titles...\n",
      "0        [seven, essay, read, sex, scene, actually, sex...\n",
      "1                                 [horrible, truth, bagel]\n",
      "2        [entire, demlib, party, infested, rat, much, c...\n",
      "3        [beginning, new, life, america, mentor, side, ...\n",
      "4        [imam, pleads, guilty, new, york, subway, bomb...\n",
      "                               ...                        \n",
      "51798    [tamil, rebel, lose, ground, sri, lanka, crisi...\n",
      "51799    [lot, people, furious, magazine, shaming, woma...\n",
      "51800    [eleven, reason, bindi, irwin, incredible, rol...\n",
      "51801    [donald, trump, backslides, campaign, promise,...\n",
      "51802                       [canadian, may, elect, senate]\n",
      "Name: title, Length: 51799, dtype: object\n",
      "Removing stopwords one more time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adammajczyk/Documents/AAA_Semy/Sem7/inzynierka/pracaInzynierska/modelling/functions.py:259: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['title'] = df['title'].apply(lambda x: [word for word in x if word not in stop_words])\n",
      "/Users/adammajczyk/Documents/AAA_Semy/Sem7/inzynierka/pracaInzynierska/modelling/functions.py:262: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"title\"] = df[\"title\"].apply(lambda x: [word for word in x if not re.match(r'^\\s*$', word)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>is_clickbait</th>\n",
       "      <th>text</th>\n",
       "      <th>dataset</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[seven, essay, read, sex, scene, actually, sex...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clickbait-dataset</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[horrible, truth, bagel]</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clickbait-dataset</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[entire, demlib, party, infested, rat, much, c...</td>\n",
       "      <td>1</td>\n",
       "      <td>— Susan? (@GaetaSusan) October 27, 2016 WikiLe...</td>\n",
       "      <td>fake-news</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[beginning, new, life, america, mentor, side, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>OAKLAND, Calif.  —   Pascal Serugendo was only...</td>\n",
       "      <td>fake-news</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[imam, pleads, guilty, new, york, subway, bomb...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clickbait-dataset</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  is_clickbait  \\\n",
       "0  [seven, essay, read, sex, scene, actually, sex...             1   \n",
       "1                           [horrible, truth, bagel]             1   \n",
       "2  [entire, demlib, party, infested, rat, much, c...             1   \n",
       "3  [beginning, new, life, america, mentor, side, ...             0   \n",
       "4  [imam, pleads, guilty, new, york, subway, bomb...             0   \n",
       "\n",
       "                                                text            dataset sample  \n",
       "0                                                NaN  clickbait-dataset  train  \n",
       "1                                                NaN  clickbait-dataset  train  \n",
       "2  — Susan? (@GaetaSusan) October 27, 2016 WikiLe...          fake-news  train  \n",
       "3  OAKLAND, Calif.  —   Pascal Serugendo was only...          fake-news  train  \n",
       "4                                                NaN  clickbait-dataset  train  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists('data/preprocessed_titles_labels.pkl'):\n",
    "    df = preprocess_title(df, verbose=True)\n",
    "    df.to_pickle('data/preprocessed_titles_labels.pkl') \n",
    "\n",
    "else:\n",
    "    df = pd.read_pickle('data/preprocessed_titles_labels.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seven',\n",
       " 'essay',\n",
       " 'read',\n",
       " 'sex',\n",
       " 'scene',\n",
       " 'actually',\n",
       " 'sexy',\n",
       " 'confident',\n",
       " 'black',\n",
       " 'men',\n",
       " 'debt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample\n",
       "train    46618\n",
       "test      2590\n",
       "val2      1296\n",
       "val1      1295\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sample'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['sample']=='train'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample\n",
       "train    46618\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['sample'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df35c0436fc4b538ad2bbe87bd3d4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current VECTOR_SIZE: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2413b74656b4010a14bc13568416690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "WINDOW:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61e1d875b4c471ebab483d1badd70fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SG:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "EPOCHS = 500\n",
    "WORKERS = 6\n",
    "MIN_COUNT = 1\n",
    "\n",
    "\n",
    "# train many word2vec models with diferent VECTOR_SIZE and WINDOW\n",
    "\n",
    "VECTOR_SIZEs = [\n",
    "    10,\n",
    "    20,\n",
    "    50,\n",
    "    100, \n",
    "    250, \n",
    "    # 500, \n",
    "    # 1000, \n",
    "   \n",
    "]\n",
    "\n",
    "WINDOWs = [\n",
    "    3, \n",
    "    4, \n",
    "    5, \n",
    "    6, \n",
    "    7, \n",
    "    8\n",
    "]\n",
    "SGs = [0, 1]\n",
    "\n",
    "\n",
    "#################################################\n",
    "# --uncomment for sample model training--\n",
    "EPOCHS = 100\n",
    "VECTOR_SIZEs = [10]\n",
    "WINDOWs = [5]\n",
    "SGs = [1]\n",
    "#################################################\n",
    "\n",
    "\n",
    "print('Start training')\n",
    "# sleep 200 ms\n",
    "time.sleep(0.2)\n",
    "\n",
    "for VECTOR_SIZE in tqdm(VECTOR_SIZEs):\n",
    "    print(f'Current VECTOR_SIZE: {VECTOR_SIZE}')\n",
    "    for WINDOW in tqdm(WINDOWs, desc=f'WINDOW'):\n",
    "        for sg in tqdm(SGs, desc=f'SG'):\n",
    "            # check if model already trained\n",
    "            if os.path.exists(f'word2vec_models/word2vec_vs{VECTOR_SIZE}_win{WINDOW}_sg{sg}.model'):\n",
    "                print(f'word2vec_vs{VECTOR_SIZE}_win{WINDOW}_sg{sg}.model already exists')\n",
    "            else:\n",
    "                model = Word2Vec(df_train['title'], vector_size=VECTOR_SIZE, window=WINDOW, min_count=MIN_COUNT, workers=WORKERS, sg=sg)\n",
    "                model.train(df_train['title'], total_examples=len(df_train['title']), epochs=EPOCHS)\n",
    "                model.save(f'word2vec_models/word2vec_vs{VECTOR_SIZE}_win{WINDOW}_sg{sg}.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('trump')\n",
    "\n",
    "# save vocabulary\n",
    "vocab = list(model.wv.index_to_key)\n",
    "\n",
    "with open('data/vocab.txt', 'w') as f:\n",
    "    for word in vocab:\n",
    "        f.write(word+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
