{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "%run '../functions.py'\n",
    "%run '../classes.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'grid_search_results_new.json'\n",
    "grids_path = '../grid_search_grids.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_settings = return_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = Word2VecModel(model_w2v_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read preprocessed data from pickle file\n",
    "df = pd.read_pickle('data/preprocessed_titles_labels.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sample'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test stratified by y\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "df['title_vector'] = [get_word_vectors(model_w2v, title, aggregation='mean') for title in df['title']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.40359393,  0.58101   ,  0.68450487, -0.9439005 ,  1.0436215 ,\n",
       "        1.4960831 , -1.5474931 , -3.395665  ,  2.0742133 ,  0.6147578 ,\n",
       "        0.06466653, -0.2790067 ,  1.0890667 , -0.5697715 ,  2.480676  ,\n",
       "       -0.36809966, -1.7050891 ,  0.20265865, -1.14607   ,  0.47789478,\n",
       "       -1.8076416 ,  0.8146902 , -0.62458795, -0.77249545, -0.47756815,\n",
       "        0.51792186,  0.3093162 ,  0.8395704 , -0.24838673,  0.5574209 ,\n",
       "       -0.24416532,  0.13703738, -0.9198161 ,  0.6077911 ,  0.37606242,\n",
       "        1.6745015 , -1.2676517 , -1.0577408 , -0.06308307,  0.47314626,\n",
       "       -1.9368892 ,  1.24171   ,  0.53468794, -1.3008615 , -1.5342575 ,\n",
       "       -0.45216206,  0.09969594,  0.79413736, -0.5304332 ,  1.0210458 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title_vector'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32,\n",
       " 31,\n",
       " 46,\n",
       " 27,\n",
       " 2,\n",
       " 17,\n",
       " 20,\n",
       " 10,\n",
       " 24,\n",
       " 34,\n",
       " 42,\n",
       " 39,\n",
       " 30,\n",
       " 6,\n",
       " 33,\n",
       " 48,\n",
       " 0,\n",
       " 22,\n",
       " 9,\n",
       " 40,\n",
       " 41,\n",
       " 7,\n",
       " 11]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables_to_drop = get_dimensions_to_drop()\n",
    "variables_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_vector'] = [drop_dimensions_from_vector(vector, variables_to_drop) for vector in df['title_vector']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.58101   , -0.9439005 ,  1.0436215 ,  1.4960831 ,  2.0742133 ,\n",
       "        1.0890667 , -0.5697715 ,  2.480676  , -0.36809966, -1.7050891 ,\n",
       "       -1.14607   ,  0.47789478,  0.8146902 , -0.77249545,  0.51792186,\n",
       "        0.3093162 , -0.24838673,  0.5574209 ,  1.6745015 , -1.2676517 ,\n",
       "       -1.0577408 , -0.06308307, -1.3008615 , -1.5342575 , -0.45216206,\n",
       "        0.79413736,  1.0210458 ], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title_vector'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.vstack(df[df['sample']=='train']['title_vector'])\n",
    "X_test = np.vstack(df[df['sample']=='val2']['title_vector'])\n",
    "\n",
    "y_train = df[df['sample']=='train']['is_clickbait']\n",
    "y_test = df[df['sample']=='val2']['is_clickbait']\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(grids_path, 'r') as f:\n",
    "    model_settings = json.load(f)\n",
    "\n",
    "model_settings\n",
    "grids = model_settings['grid_search_grids']\n",
    "grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decision_tree', 'catboost', 'lightgbm', 'xgboost', 'random_forest']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_to_CV = list()\n",
    "\n",
    "for model_name, grid in grids.items():\n",
    "    # check if model has already been trained - if 'best_params' exists in grid\n",
    "    if 'best_params' in grid.keys():\n",
    "        print(f'Model {model_name} already trained')\n",
    "        continue\n",
    "    models_to_CV.append((model_name))\n",
    "models_to_CV\n",
    "\n",
    "# remove _grid from model names\n",
    "models_to_CV = [model_name.replace('_grid', '') for model_name in models_to_CV]\n",
    "models_to_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_model_with_param(model_name, param = dict()):\n",
    "    if model_name == 'decision_tree':\n",
    "        model = DecisionTreeClassifier(**param)\n",
    "    elif model_name == 'random_forest':\n",
    "        model = RandomForestClassifier(**param)\n",
    "    elif model_name == 'xgboost':\n",
    "        model = XGBClassifier(**param)\n",
    "    elif model_name == 'lightgbm':\n",
    "        model = LGBMClassifier(**param, verbose=-1)\n",
    "    elif model_name == 'catboost':\n",
    "        model = CatBoostClassifier(**param,verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_CV = [\n",
    "    'xgboost',\n",
    "    'lightgbm', \n",
    "    'decision_tree', \n",
    "    'random_forest',  \n",
    "    'catboost'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# read existing results\n",
    "try:\n",
    "    with open(save_path, 'r') as f:\n",
    "        grid_search_results = json.load(f)\n",
    "    print('Loaded existing results')\n",
    "except:\n",
    "    grid_search_results = dict()\n",
    "    print('No existing results found - creating new dict')\n",
    "for model_name in models_to_CV:\n",
    "    grid_search_results[model_name] = dict()\n",
    "\n",
    "for model_name in tqdm(models_to_CV, desc = 'Models'):\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # get grid\n",
    "    grid = grids[model_name+'_grid']\n",
    "\n",
    "    # generate all combinations of parameters\n",
    "    import itertools\n",
    "\n",
    "    keys, values = zip(*grid.items())\n",
    "    combinations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    # do grid search\n",
    "    \n",
    "    for params in combinations_dicts:\n",
    "        grid_search_results[model_name][str(params)] = dict()\n",
    "\n",
    "    for params in tqdm(combinations_dicts, desc = 'Grid combinations search for model {}'.format(model_name)):\n",
    "       \n",
    "        model = return_model_with_param(model_name, params)\n",
    "        scores_auc_cv_val = list()\n",
    "        scores_auc_val2 = list()\n",
    "        scores_auc_train = list()\n",
    "\n",
    "        scores_f1_cv_val = list()\n",
    "        scores_f1_val2 = list()\n",
    "        scores_f1_train = list()\n",
    "\n",
    "        for train_index, val_index in kf.split(X_train_scaled, y_train):\n",
    "            X_train_kf = X_train_scaled[train_index]\n",
    "            X_val_kf = X_train_scaled[val_index]\n",
    "            y_train_kf = y_train.iloc[train_index]\n",
    "            y_val_kf = y_train.iloc[val_index]\n",
    "\n",
    "            model.fit(X_train_kf, y_train_kf)\n",
    "\n",
    "            # print(model)\n",
    "\n",
    "        \n",
    "            scores_auc_cv_val.append(roc_auc_score(y_val_kf, model.predict_proba(X_val_kf)[:,1]))\n",
    "            scores_auc_val2.append(roc_auc_score(y_test, model.predict_proba(X_test_scaled)[:,1]))\n",
    "            scores_auc_train.append(roc_auc_score(y_train_kf, model.predict_proba(X_train_kf)[:,1]))\n",
    "\n",
    "            y_pred_cv_val = model.predict_proba(X_val_kf)[:,1]\n",
    "            y_pred_val2 = model.predict_proba(X_test_scaled)[:,1]\n",
    "            y_pred_train = model.predict_proba(X_train_kf)[:,1]\n",
    "\n",
    "            # print(y_pred_cv_val[y_pred_cv_val>0.5])\n",
    "\n",
    "            y_pred_cv_val = np.where(y_pred_cv_val > 0.5, 1, 0)\n",
    "            y_pred_val2 = np.where(y_pred_val2 > 0.5, 1, 0)\n",
    "            y_pred_train = np.where(y_pred_train > 0.5, 1, 0)\n",
    "\n",
    "\n",
    "            \n",
    "            scores_f1_cv_val.append(f1_score(y_val_kf, model.predict(X_val_kf)))\n",
    "            scores_f1_val2.append(f1_score(y_test, model.predict(X_test_scaled)))\n",
    "            scores_f1_train.append(f1_score(y_train_kf, model.predict(X_train_kf)))\n",
    "            break\n",
    "\n",
    "        # print(scores_f1_train, scores_f1_cv_val, scores_f1_val2)\n",
    "        # print(scores_auc_train, scores_auc_cv_val, scores_auc_val2)\n",
    "\n",
    "        grid_search_results[model_name][str(params)]['mean_train_f1'] = np.mean(scores_f1_train)\n",
    "        grid_search_results[model_name][str(params)]['mean_cv_val_f1'] = np.mean(scores_f1_cv_val)\n",
    "        grid_search_results[model_name][str(params)]['mean_val2_f1'] = np.mean(scores_f1_val2)\n",
    "\n",
    "        grid_search_results[model_name][str(params)]['mean_train_auc'] = np.mean(scores_auc_train)\n",
    "        grid_search_results[model_name][str(params)]['mean_cv_val_auc'] = np.mean(scores_auc_cv_val)\n",
    "        grid_search_results[model_name][str(params)]['mean_val2_auc'] = np.mean(scores_auc_val2)\n",
    "\n",
    "        grid_search_results[model_name][str(params)]['f1_diff_cv_val'] = np.mean(scores_f1_train) - np.mean(scores_f1_cv_val)\n",
    "        grid_search_results[model_name][str(params)]['auc_diff_cv_val'] = np.mean(scores_auc_train) - np.mean(scores_auc_cv_val)\n",
    "\n",
    "        grid_search_results[model_name][str(params)]['f1_diff_val2'] = np.mean(scores_f1_train) - np.mean(scores_f1_val2)\n",
    "        grid_search_results[model_name][str(params)]['auc_diff_val2'] = np.mean(scores_auc_train) - np.mean(scores_auc_val2)\n",
    "\n",
    "        grid_search_results[model_name][str(params)]['params'] = params\n",
    "\n",
    "        # save results to json file\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(grid_search_results, f)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    print(grid_search_results[model_name])\n",
    "    \n",
    "# grid_search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "with open(save_path, 'w') as f:\n",
    "    json.dump(grid_search_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results\n",
    "with open(save_path, 'r') as f:\n",
    "    grid_search_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res= pd.DataFrame()\n",
    "\n",
    "for model in grid_search_results:\n",
    "    temp = grid_search_results[model]\n",
    "    temp = pd.DataFrame.from_dict(temp)\n",
    "    temp = temp.transpose()\n",
    "    temp['model'] = model\n",
    "    df_res = pd.concat([df_res, temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = df_res.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_auc = df_res[(df_res['auc_diff_val2']<=0.05)].sort_values(by='mean_val2_auc', ascending=False).head(500)\n",
    "top_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topf1 = df_res[(df_res['f1_diff_val2']<=0.05)].sort_values(by='mean_val2_f1', ascending=False).head(500)\n",
    "topf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_both = pd.merge(top_auc, topf1, on=['index'], how='inner')\n",
    "\n",
    "# keep _x columns\n",
    "top_both = top_both[[col for col in top_both.columns if '_x' in col]]\n",
    "top_both.columns = [col.replace('_x', '') for col in top_both.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_both['sum_diff_val2'] = top_both['f1_diff_val2'] + top_both['auc_diff_val2']\n",
    "top_both['sum_auc_f1_val2'] = top_both['mean_val2_f1'] + top_both['mean_val2_auc']\n",
    "\n",
    "top_both = top_both.sort_values(by='sum_auc_f1_val2', ascending=False).head(1000).reset_index(drop=True)\n",
    "top_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner model: |catboost| with params: {'iterations': 100, 'learning_rate': 0.2, 'depth': 8, 'l2_leaf_reg': 5}\n"
     ]
    }
   ],
   "source": [
    "params, model_name = top_both['params'][0], top_both['model'][0]\n",
    "print('Winner model: |{}|'.format(model_name), 'with params: {}'.format(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x2a90ff340>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model with best params\n",
    "model = return_model_with_param(model_name, params)\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import pickle\n",
    "with open('predictive_models/{}.pkl'.format(model_name), 'wb') as f:\n",
    "    pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save scaler\n",
    "with open('predictive_models/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8602941176470589\n",
      "AUC score: 0.9433612355639807\n"
     ]
    }
   ],
   "source": [
    "# calculate metrics on test set\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "X_test = np.vstack(df[df['sample']=='test']['title_vector'])\n",
    "y_test = df[df['sample']=='test']['is_clickbait']\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "print('F1 score: {}'.format(f1_score(y_test, y_pred)))\n",
    "print('AUC score: {}'.format(roc_auc_score(y_test, y_pred_proba)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best threshold for f1 score\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "# find threshold for best f1 score\n",
    "thresholds = np.linspace(0, 1, 10000)\n",
    "f1s = dict()\n",
    "for threshold in tqdm(thresholds):\n",
    "    y_pred = (model.predict_proba(X_test_scaled)[:, 1] >= threshold).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    f1s[threshold] = f1\n",
    "\n",
    "# get threshold for best f1 score\n",
    "threshold = max(f1s, key=f1s.get)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC test: 0.9433612355639807\n",
      "F1 test: 0.8673114119922631\n"
     ]
    }
   ],
   "source": [
    "# find auc and f1 for best threshold\n",
    "y_pred = (model.predict_proba(X_test_scaled)[:, 1] >= threshold).astype(int)\n",
    "y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'ROC AUC test: {roc_auc}')\n",
    "print(f'F1 test: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC train: 0.9734385750508231\n",
      "F1 train: 0.9096811644056502\n"
     ]
    }
   ],
   "source": [
    "# find auc and f1 for best threshold on train\n",
    "y_pred = (model.predict_proba(X_train_scaled)[:, 1] >= threshold).astype(int)\n",
    "y_proba = model.predict_proba(X_train_scaled)[:, 1]\n",
    "\n",
    "roc_auc = roc_auc_score(y_train, y_proba)\n",
    "f1 = f1_score(y_train, y_pred)\n",
    "\n",
    "print(f'ROC AUC train: {roc_auc}')\n",
    "print(f'F1 train: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save threshold to txt file\n",
    "with open('predictive_models/threshold.txt', 'w') as f:\n",
    "    f.write(str(threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dim_1', 'dim_3', 'dim_4', 'dim_5', 'dim_8', 'dim_12', 'dim_13',\n",
       "       'dim_14', 'dim_15', 'dim_16', 'dim_18', 'dim_19', 'dim_21',\n",
       "       'dim_23', 'dim_25', 'dim_26', 'dim_28', 'dim_29', 'dim_35',\n",
       "       'dim_36', 'dim_37', 'dim_38', 'dim_43', 'dim_44', 'dim_45',\n",
       "       'dim_47', 'dim_49'], dtype='<U6')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_undropped_dimensions(dropped_dimensions, n_dim):\n",
    "    all_dimensions = list(range(n_dim))\n",
    "    undropped_dimensions = [dim for dim in all_dimensions if dim not in dropped_dimensions]\n",
    "    return undropped_dimensions\n",
    "\n",
    "undropped_dimensions = get_undropped_dimensions(variables_to_drop, 50)\n",
    "undropped_dimensions = [str('dim_'+str(dim)) for dim in undropped_dimensions]\n",
    "undropped_dimensions = np.array(undropped_dimensions)\n",
    "undropped_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=undropped_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparation of a new explainer is initiated\n",
      "\n",
      "  -> data              : 46618 rows 27 cols\n",
      "  -> target variable   : Parameter 'y' was a pandas.Series. Converted to a numpy.ndarray.\n",
      "  -> target variable   : 46618 values\n",
      "  -> model_class       : catboost.core.CatBoostClassifier (default)\n",
      "  -> label             : Champion Model\n",
      "  -> predict function  : <function yhat_proba_default at 0x2b3df44c0> will be used (default)\n",
      "  -> predict function  : Accepts pandas.DataFrame and numpy.ndarray.\n",
      "  -> predicted values  : min = 0.000246, mean = 0.49, max = 1.0\n",
      "  -> model type        : classification will be used (default)\n",
      "  -> residual function : difference between y and yhat (default)\n",
      "  -> residuals         : min = -0.994, mean = 0.000203, max = 0.996\n",
      "  -> model_info        : package catboost\n",
      "\n",
      "A new explainer has been created!\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "displaylogo": false,
        "modeBarButtonsToRemove": [
         "sendDataToCloud",
         "lasso2d",
         "autoScale2d",
         "select2d",
         "zoom2d",
         "pan2d",
         "zoomIn2d",
         "zoomOut2d",
         "resetScale2d",
         "toggleSpikelines",
         "hoverCompareCartesian",
         "hoverClosestCartesian"
        ],
        "plotlyServerURL": "https://plot.ly",
        "staticPlot": false,
        "toImageButtonOptions": {
         "height": null,
         "width": null
        }
       },
       "data": [
        {
         "base": 0.026810104402304736,
         "hoverinfo": "text",
         "hoverlabel": {
          "bgcolor": "rgba(0,0,0,0.8)"
         },
         "hovertext": [
          "Model: Champion Model loss after<br>variable: dim_26 is permuted: 0.045<br>Drop-out loss change: +0.018",
          "Model: Champion Model loss after<br>variable: dim_36 is permuted: 0.042<br>Drop-out loss change: +0.015",
          "Model: Champion Model loss after<br>variable: dim_45 is permuted: 0.038<br>Drop-out loss change: +0.011",
          "Model: Champion Model loss after<br>variable: dim_16 is permuted: 0.037<br>Drop-out loss change: +0.01",
          "Model: Champion Model loss after<br>variable: dim_28 is permuted: 0.036<br>Drop-out loss change: +0.009",
          "Model: Champion Model loss after<br>variable: dim_12 is permuted: 0.036<br>Drop-out loss change: +0.009",
          "Model: Champion Model loss after<br>variable: dim_37 is permuted: 0.036<br>Drop-out loss change: +0.009",
          "Model: Champion Model loss after<br>variable: dim_15 is permuted: 0.034<br>Drop-out loss change: +0.008",
          "Model: Champion Model loss after<br>variable: dim_49 is permuted: 0.034<br>Drop-out loss change: +0.007",
          "Model: Champion Model loss after<br>variable: dim_5 is permuted: 0.034<br>Drop-out loss change: +0.007",
          "Model: Champion Model loss after<br>variable: dim_18 is permuted: 0.034<br>Drop-out loss change: +0.007",
          "Model: Champion Model loss after<br>variable: dim_19 is permuted: 0.033<br>Drop-out loss change: +0.007",
          "Model: Champion Model loss after<br>variable: dim_3 is permuted: 0.033<br>Drop-out loss change: +0.006",
          "Model: Champion Model loss after<br>variable: dim_8 is permuted: 0.033<br>Drop-out loss change: +0.006",
          "Model: Champion Model loss after<br>variable: dim_35 is permuted: 0.032<br>Drop-out loss change: +0.005",
          "Model: Champion Model loss after<br>variable: dim_38 is permuted: 0.032<br>Drop-out loss change: +0.005",
          "Model: Champion Model loss after<br>variable: dim_1 is permuted: 0.032<br>Drop-out loss change: +0.005",
          "Model: Champion Model loss after<br>variable: dim_29 is permuted: 0.031<br>Drop-out loss change: +0.005",
          "Model: Champion Model loss after<br>variable: dim_23 is permuted: 0.031<br>Drop-out loss change: +0.005",
          "Model: Champion Model loss after<br>variable: dim_21 is permuted: 0.031<br>Drop-out loss change: +0.004",
          "Model: Champion Model loss after<br>variable: dim_43 is permuted: 0.031<br>Drop-out loss change: +0.004",
          "Model: Champion Model loss after<br>variable: dim_47 is permuted: 0.031<br>Drop-out loss change: +0.004",
          "Model: Champion Model loss after<br>variable: dim_13 is permuted: 0.031<br>Drop-out loss change: +0.004",
          "Model: Champion Model loss after<br>variable: dim_14 is permuted: 0.031<br>Drop-out loss change: +0.004",
          "Model: Champion Model loss after<br>variable: dim_44 is permuted: 0.03<br>Drop-out loss change: +0.003",
          "Model: Champion Model loss after<br>variable: dim_25 is permuted: 0.03<br>Drop-out loss change: +0.003",
          "Model: Champion Model loss after<br>variable: dim_4 is permuted: 0.03<br>Drop-out loss change: +0.003"
         ],
         "marker": {
          "color": "#46bac2"
         },
         "orientation": "h",
         "showlegend": false,
         "text": [
          "+0.018",
          "+0.015",
          "+0.011",
          "+0.01",
          "+0.009",
          "+0.009",
          "+0.009",
          "+0.008",
          "+0.007",
          "+0.007",
          "+0.007",
          "+0.007",
          "+0.006",
          "+0.006",
          "+0.005",
          "+0.005",
          "+0.005",
          "+0.005",
          "+0.005",
          "+0.004",
          "+0.004",
          "+0.004",
          "+0.004",
          "+0.004",
          "+0.003",
          "+0.003",
          "+0.003"
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          0.018124841611000742,
          0.015287650962120935,
          0.01069937854007422,
          0.010137697264896762,
          0.009181059709954618,
          0.008857903665987352,
          0.008763397141689066,
          0.007505301293601465,
          0.007140799043666959,
          0.006790646159350742,
          0.006713097247314646,
          0.006535377888199554,
          0.0062783524618579035,
          0.006209959731255401,
          0.00508927895004364,
          0.005055521417837595,
          0.004690803231564556,
          0.004573674125246032,
          0.004504471006970868,
          0.004470542634024532,
          0.004437582864134402,
          0.004362831763351636,
          0.004111030429796936,
          0.003946430300422053,
          0.0033829154032453783,
          0.0032634069666381826,
          0.0028966610632516245
         ],
         "xaxis": "x",
         "y": [
          "dim_26",
          "dim_36",
          "dim_45",
          "dim_16",
          "dim_28",
          "dim_12",
          "dim_37",
          "dim_15",
          "dim_49",
          "dim_5",
          "dim_18",
          "dim_19",
          "dim_3",
          "dim_8",
          "dim_35",
          "dim_38",
          "dim_1",
          "dim_29",
          "dim_23",
          "dim_21",
          "dim_43",
          "dim_47",
          "dim_13",
          "dim_14",
          "dim_44",
          "dim_25",
          "dim_4"
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Champion Model",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "drop-out loss",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0,
          "yanchor": "top",
          "yref": "paper",
          "yshift": -30
         }
        ],
        "font": {
         "color": "#371ea3"
        },
        "height": 723,
        "margin": {
         "b": 71,
         "r": 30,
         "t": 78
        },
        "shapes": [
         {
          "line": {
           "color": "#371ea3",
           "dash": "dot",
           "width": 1.5
          },
          "type": "line",
          "x0": 0.026810104402304736,
          "x1": 0.026810104402304736,
          "xref": "x",
          "y0": -1,
          "y1": 27,
          "yref": "y"
         }
        ],
        "template": {
         "data": {
          "scatter": [
           {
            "type": "scatter"
           }
          ]
         }
        },
        "title": {
         "text": "Variable Importance",
         "x": 0.15
        },
        "xaxis": {
         "anchor": "y",
         "automargin": true,
         "domain": [
          0,
          1
         ],
         "fixedrange": true,
         "gridwidth": 2,
         "range": [
          0.024091378160654625,
          0.04765367225495559
         ],
         "tickcolor": "white",
         "ticklen": 3,
         "ticks": "outside",
         "type": "linear",
         "zeroline": false
        },
        "yaxis": {
         "anchor": "x",
         "automargin": true,
         "autorange": "reversed",
         "domain": [
          0,
          1
         ],
         "fixedrange": true,
         "gridwidth": 2,
         "tickcolor": "white",
         "ticklen": 10,
         "ticks": "outside",
         "type": "category"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dalex\n",
    "# Create an explainer object\n",
    "exp = dalex.Explainer(model, X_train_scaled_df, y_train, label='Champion Model')\n",
    "\n",
    "# Calculate feature importance\n",
    "feature_importance = exp.model_parts()\n",
    "feature_importance.plot(max_vars=27)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
