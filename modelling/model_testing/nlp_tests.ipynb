{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adamm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\adamm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adamm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\adamm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\adamm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run -i \"../classes.py\"\n",
    "%run -i \"../functions.py\"\n",
    "\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"../all_datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_settings_all_datasets = {\n",
    "     \"best_word2vec\":\n",
    "    {\n",
    "        \"model_path\": \"../all_datasets/word2vec_models/word2vec_vs50_win5_sg0.model\",\n",
    "        \"vector_size\": 50,\n",
    "        \"window_size\": 5,\n",
    "        \"is_skipgram\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "model_settings_clickbait_dataset = {\n",
    "     \"best_word2vec\":\n",
    "    {\n",
    "        \"model_path\": \"../all_datasets/word2vec_models/word2vec_vs20_win4_sg1.model\",\n",
    "        \"vector_size\": 20,\n",
    "        \"window_size\": 4,\n",
    "        \"is_skipgram\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "model_settings_fake_news = {\n",
    "     \"best_word2vec\":\n",
    "    {\n",
    "        \"model_path\": \"../all_datasets/word2vec_models/word2vec_vs100_win3_sg0.model\",\n",
    "        \"vector_size\": 100,\n",
    "        \"window_size\": 3,\n",
    "        \"is_skipgram\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "model_w2v_all_datasets = Word2VecModel(model_settings_all_datasets[\"best_word2vec\"])\n",
    "model_w2v_clickbait_dataset = Word2VecModel(model_settings_clickbait_dataset[\"best_word2vec\"])\n",
    "model_w2v_fake_news = Word2VecModel(model_settings_fake_news[\"best_word2vec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../clickbait_dataset/predictive_models/worst_performing_dimensions_intersection.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m variables_to_drop_all_datasets \u001b[38;5;241m=\u001b[39m get_dimensions_to_drop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../all_datasets/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m variables_to_drop_clickbait_dataset \u001b[38;5;241m=\u001b[39m get_dimensions_to_drop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../clickbait_dataset/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m variables_to_drop_fake_news \u001b[38;5;241m=\u001b[39m get_dimensions_to_drop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../fake_news_dataset/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\sheeesh\\pracaInzynierska\\modelling\\functions.py:279\u001b[0m, in \u001b[0;36mget_dimensions_to_drop\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dimensions_to_drop\u001b[39m(path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# read variables to be dropped from pickle file\u001b[39;00m\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictive_models/worst_performing_dimensions_intersection.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    280\u001b[0m         variables_to_drop \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m    282\u001b[0m     variables_to_drop \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m variables_to_drop]\n",
      "File \u001b[1;32mc:\\Users\\adamm\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../clickbait_dataset/predictive_models/worst_performing_dimensions_intersection.pkl'"
     ]
    }
   ],
   "source": [
    "variables_to_drop_all_datasets = get_dimensions_to_drop(\"../all_datasets/\")\n",
    "variables_to_drop_clickbait_dataset = get_dimensions_to_drop(\"../clickbait_dataset/\")\n",
    "variables_to_drop_fake_news = get_dimensions_to_drop(\"../fake_news_dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>is_clickbait</th>\n",
       "      <th>text</th>\n",
       "      <th>dataset</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[florida, back, away, prep, schedule, favoring...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clickbait-dataset</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[beck, comey, letter,  , one, irresponsible, t...</td>\n",
       "      <td>1</td>\n",
       "      <td>Home › POLITICS › BECK: COMEY LETTER ‘ONE OF T...</td>\n",
       "      <td>fake-news</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[sydney, swelter, climate, change, link, scien...</td>\n",
       "      <td>0</td>\n",
       "      <td>Southeastern Australia has suffered through a ...</td>\n",
       "      <td>fake-news</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[california, today, airbnb, hometown, success,...</td>\n",
       "      <td>0</td>\n",
       "      <td>Good morning. This is the last day of our test...</td>\n",
       "      <td>fake-news</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[clinton, could, win, control, voting, machine...</td>\n",
       "      <td>1</td>\n",
       "      <td>Voter Fraud is Trump’s Greatest Challenge New ...</td>\n",
       "      <td>fake-news</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  is_clickbait  \\\n",
       "0  [florida, back, away, prep, schedule, favoring...             0   \n",
       "1  [beck, comey, letter,  , one, irresponsible, t...             1   \n",
       "2  [sydney, swelter, climate, change, link, scien...             0   \n",
       "3  [california, today, airbnb, hometown, success,...             0   \n",
       "4  [clinton, could, win, control, voting, machine...             1   \n",
       "\n",
       "                                                text            dataset sample  \n",
       "0                                                NaN  clickbait-dataset   test  \n",
       "1  Home › POLITICS › BECK: COMEY LETTER ‘ONE OF T...          fake-news   test  \n",
       "2  Southeastern Australia has suffered through a ...          fake-news   test  \n",
       "3  Good morning. This is the last day of our test...          fake-news   test  \n",
       "4  Voter Fraud is Trump’s Greatest Challenge New ...          fake-news   test  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/preprocessed_titles_labels.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df=df[df['sample']=='test'].reset_index(drop=True)\n",
    "\n",
    "df['title_vector_all_datasets'] = [get_word_vectors(model_w2v_all_datasets, title, aggregation='mean') for title in df['title']]\n",
    "df['title_vector_clickbait-dataset'] = [get_word_vectors(model_w2v_clickbait_dataset, title, aggregation='mean') for title in df['title']]\n",
    "df['title_vector_fake-news'] = [get_word_vectors(model_w2v_fake_news, title, aggregation='mean') for title in df['title']]\n",
    "\n",
    "df['title_vector_all_datasets'] = [drop_dimensions_from_vector(vector, variables_to_drop_all_datasets) for vector in df['title_vector_all_datasets']]\n",
    "df['title_vector_clickbait-dataset'] = [drop_dimensions_from_vector(vector, variables_to_drop_clickbait_dataset) for vector in df['title_vector_clickbait-dataset']]\n",
    "df['title_vector_fake-news'] = [drop_dimensions_from_vector(vector, variables_to_drop_fake_news) for vector in df['title_vector_fake-news']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2590, 5), (1595, 5), (995, 5))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy as cp\n",
    "df_all_datasets = cp.deepcopy(df)\n",
    "df_clickbait_dataset = df[df['dataset']=='clickbait-dataset'].reset_index(drop=True)\n",
    "df_fake_news = df[df['dataset']=='fake-news'].reset_index(drop=True)\n",
    "\n",
    "df_all_datasets.shape, df_clickbait_dataset.shape, df_fake_news.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_combinations = ['mod_all_datasets', 'mod_fake-news', 'mod_clickbait-dataset-small', 'mod_clickbait-dataset-large']\n",
    "dataset_combinations = ['data_all_datasets', 'data_fake-news', 'data_clickbait-dataset']\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1-score', 'roc-auc']\n",
    "tresholds = ['default', 'optimizedF1']\n",
    "\n",
    "results = dict()\n",
    "\n",
    "for model in model_combinations:\n",
    "    results[model] = dict()\n",
    "    for dataset in dataset_combinations:\n",
    "        results[model][dataset] = dict()\n",
    "        for threshold in tresholds:\n",
    "            results[model][dataset][threshold] = dict()\n",
    "            for metric in metrics:\n",
    "                results[model][dataset][threshold][metric] = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../all_datasets/predictive_models/catboost.pkl', 'rb') as f:\n",
    "    model_all_datasets = pickle.load(f)\n",
    "\n",
    "with open('../clickbait_dataset/predictive_models/catboost.pkl', 'rb') as f:\n",
    "    model_clickbait_dataset_small = pickle.load(f)\n",
    "\n",
    "with open('../clickbait_dataset_more_variables/predictive_models/lightgbm.pkl', 'rb') as f:\n",
    "    model_clickbait_dataset_large = pickle.load(f)\n",
    "\n",
    "with open('../fake_news/predictive_models/catboost.pkl', 'rb') as f:\n",
    "    model_fake_news = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../all_datasets/predictive_models/threshold.txt', 'rb') as f:\n",
    "    threshold_all_datasets = float(f.read())\n",
    "\n",
    "with open('../clickbait_dataset/predictive_models/threshold.txt', 'rb') as f:\n",
    "    threshold_clickbait_dataset_small = float(f.read())\n",
    "\n",
    "with open('../clickbait_dataset_more_variables/predictive_models/threshold.txt', 'rb') as f:\n",
    "    threshold_clickbait_dataset_large = float(f.read())\n",
    "\n",
    "with open('../fake_news/predictive_models/threshold.txt', 'rb') as f:\n",
    "    threshold_fake_news = float(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adamm\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open('../all_datasets/predictive_models/scaler.pkl', 'rb') as f:\n",
    "    scaler_all_datasets = pickle.load(f)\n",
    "\n",
    "with open('../clickbait_dataset/predictive_models/scaler.pkl', 'rb') as f:\n",
    "    scaler_clickbait_dataset_small = pickle.load(f)\n",
    "\n",
    "with open('../clickbait_dataset_more_variables/predictive_models/scaler.pkl', 'rb') as f:\n",
    "    scaler_clickbait_dataset_large = pickle.load(f)\n",
    "\n",
    "with open('../fake_news/predictive_models/scaler.pkl', 'rb') as f:\n",
    "    scaler_fake_news = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All datasets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "for model in tqdm(model_combinations, desc='models'):\n",
    "    if model == 'mod_all_datasets':\n",
    "        scaler = cp.deepcopy(scaler_all_datasets)\n",
    "        predictor = cp.deepcopy(model_all_datasets)\n",
    "        opt_threshold = threshold_all_datasets\n",
    "    elif model == 'mod_fake-news':\n",
    "        scaler = cp.deepcopy(scaler_fake_news)\n",
    "        predictor = cp.deepcopy(model_fake_news)\n",
    "        opt_threshold = threshold_fake_news\n",
    "    elif model == 'mod_clickbait-dataset-small':\n",
    "        scaler = cp.deepcopy(scaler_clickbait_dataset_small)\n",
    "        predictor = cp.deepcopy(model_clickbait_dataset_small)\n",
    "        opt_threshold = threshold_clickbait_dataset_small\n",
    "    elif model == 'mod_clickbait-dataset-large':\n",
    "        scaler = cp.deepcopy(scaler_clickbait_dataset_large)\n",
    "        predictor = cp.deepcopy(model_clickbait_dataset_large)\n",
    "        opt_threshold = threshold_clickbait_dataset_large\n",
    "       \n",
    "\n",
    "    for dataset in tqdm(dataset_combinations, desc='datasets'):\n",
    "        if dataset == 'data_all_datasets':\n",
    "            df_temp = cp.deepcopy(df_all_datasets)\n",
    "            dataset_name = 'all_datasets'\n",
    "        elif dataset == 'data_fake-news':\n",
    "            df_temp = cp.deepcopy(df_fake_news)\n",
    "            dataset_name = 'fake-news'\n",
    "        elif dataset == 'data_clickbait-dataset':\n",
    "            df_temp = cp.deepcopy(df_clickbait_dataset)\n",
    "            dataset_name = 'clickbait-dataset'\n",
    "\n",
    "        for threhold, threshold_name in zip([0.5, opt_threshold], ['default', 'optimizedF1']):\n",
    "            X = np.vstack(df_temp['title_vector_' + dataset_name].values)\n",
    "            y_true = df_temp['is_clickbait']\n",
    "            X_scaled = scaler.transform(X)\n",
    "\n",
    "            y_pred_proba = predictor.predict_proba(X_scaled)[:,1]\n",
    "            y_pred = (y_pred_proba > threhold).astype(int)\n",
    "\n",
    "\n",
    "            results[model][dataset][threshold_name]['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "            results[model][dataset][threshold_name]['precision'] = precision_score(y_true, y_pred)\n",
    "            results[model][dataset][threshold_name]['recall'] = recall_score(y_true, y_pred)\n",
    "            results[model][dataset][threshold_name]['f1-score'] = f1_score(y_true, y_pred)\n",
    "            results[model][dataset][threshold_name]['roc-auc'] = roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
